# 机器学习 

( 王天一专栏  记录)

在内容上，“机器学习”分为 3 个模块。

1. **机器学习概观**，介绍机器学习中超脱于具体模型和方法之上的一些共性问题，将从概率的两大派别开始。众所周知，概率在机器学习中扮演着核心角色，而频率学派与贝叶斯学派对概率迥异的认知也将机器学习一分为二，发展出两套完全不同的理论体系。正所谓兼听则明偏听则暗，理解机器学习时应该看到这同一枚硬币的两面，以获得完整的认知。除此之外，本模块还涵盖了计算学习等机器学习的理论问题，以及关于模型和特征的一些实验主题。

2. 讨论频率学派发展出的机器学习理论——统计学习。**统计机器学习**的核心是数据，它既从数据中来，利用不同的模型去拟合数据背后的规律；也到数据中去，用拟合出的规律去推断和预测未知的结果。统计学习中最基础的模型是线性回归，几乎所有其他模型都是从不同角度对线性回归模型做出的扩展与修正。因此，在这个模块中，我将以**线性模型**为主线，和你一起浏览它的万千变化，观察从简单线性回归到复杂深度网络的发展历程。

3. 讨论贝叶斯学派发展出的机器学习理论——符号学习，也就是**概率图模型**。和基于数据的统计学习相比，基于关系的图模型更多地代表了因果推理的发展方向。贝叶斯主义也需要计算待学习对象的概率分布，但它利用的不是海量的具体数据，而是变量之间的相关关系、每个变量的先验分布和大量复杂的积分技巧。在这个模块中，我将围绕概率图模型中的**表示、推断、学习**三大问题展开介绍，认识贝叶斯面纱下的机器学习。





## 频率视角下的机器学习

“概率”（probability）这个基本概念存在着两种解读方式，它们分别对应着**概率的频率学派**（Frequentist）和**贝叶斯学派**（Bayesian）。而解读方式上的差异也延伸到了以概率为基础的其他学科，尤其是机器学习之中。



**频率学派口中的概率表示的是事件发生频率的极限值**，它只有在无限次的独立重复试验之下才有绝对的精确意义。

**在频率学派眼中，当重复试验的次数趋近于无穷大时，事件发生的频率会收敛到真实的概率之上。这种观点背后暗含了一个前提，那就是概率是一个确定的值，并不会受单次观察结果的影响。**



**频率统计理论的核心在于认定待估计的参数是固定不变的常量，讨论参数的概率分布是没有意义的；而用来估计参数的数据是随机的变量，每个数据都是参数支配下一次独立重复试验的结果。由于参数本身是确定的，那频率的波动就并非来源于参数本身的不确定性，而是由有限次观察造成的干扰而导致**。



统计学的核⼼任务之一是根据从总体中抽取出的样本，也就是数据来估计未知的总体参数。参数的最优估计可以通过样本数据的分布，也就是**采样分布**（sampling distribution）来求解，由于频率统计将数据看作随机变量，所以计算采样分布是没有问题的。确定采样分布之后，参数估计可以等效成一个最优化的问题，而频率统计最常使用的最优化方法，就是**最大似然估计**（maximum likelihood estimation）。



**回忆一下最大似然估计，它的目标是让似然概率最大化，也就是固定参数的前提之下，数据出现的条件概率最大化**。这是频率学派估计参数的基本出发点：一组数据之所以能够在单次试验中出现，是因为它出现的可能性最大。而参数估计的过程就是赋予观测数据最大似然概率的过程。这可以通过下面这个简单的例子来说明：

“如果观测到的数据 $θi$ 是真实值 $θ$ 和方差为 $σ2$，但形式未知的噪声 eiei 的叠加，那么如何得出 θθ 的最优估计值？”

要用最大似然估计解决这个问题，首先就要对似然概率进行建模，建模中的一个重要假设是假定未知形式的噪声满足高斯分布。这不仅在统计学中，在其他学科里也是一个常用的假设。

从理论上说，在功率有限的条件下，高斯噪声的信源熵最大，因而带来的不确定性也就越大，换句话说，这是最恶劣的噪声；从实践上说，真实的噪声通常来源于多个独立的物理过程，都具有不同的概率分布，中心极限定理告诉我们，当噪声源的数目越来越多时，它们的叠加就趋近于高斯分布，因而高斯噪声就是对真实情况的一个合理的模拟。

在高斯噪声的假设下，每个观测数据 θiθi 所满足的概率分布就可以写成


$$

$$








![img](https://static001.geekbang.org/resource/image/a7/58/a7a64ab55c83c7a1c2519a6dc777cb58.jpg)



















